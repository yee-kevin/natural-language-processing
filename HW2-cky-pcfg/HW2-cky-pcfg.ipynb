{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 50.040 Natural Language Processing (Summer 2020) Homework 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "from collections import Counter\n",
    "from nltk.tree import Tree\n",
    "from nltk import Nonterminal\n",
    "from nltk.corpus import LazyCorpusLoader, BracketParseCorpusReader\n",
    "from collections import defaultdict\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "st = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package treebank to\n",
      "[nltk_data]     C:\\Users\\kevin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package treebank is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('treebank')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_leave_lower(tree_string):\n",
    "    if isinstance(tree_string, Tree):\n",
    "        tree = tree_string\n",
    "    else:\n",
    "        tree = Tree.fromstring(tree_string)\n",
    "    for idx, _ in enumerate(tree.leaves()):\n",
    "        tree_location = tree.leaf_treeposition(idx)\n",
    "        non_terminal = tree[tree_location[:-1]]\n",
    "        non_terminal[0] = non_terminal[0].lower()\n",
    "    return tree\n",
    "\n",
    "def get_train_test_data():\n",
    "    '''\n",
    "    Load training and test set from nltk corpora\n",
    "    '''\n",
    "    train_num = 3900\n",
    "    test_index = range(10)\n",
    "    treebank = LazyCorpusLoader('treebank/combined', BracketParseCorpusReader, r'wsj_.*\\.mrg')\n",
    "    cnf_train = treebank.parsed_sents()[:train_num]\n",
    "    cnf_test = [treebank.parsed_sents()[i+train_num] for i in test_index]\n",
    "    #Convert to Chomsky norm form, remove auxiliary labels\n",
    "    cnf_train = [convert2cnf(t) for t in cnf_train]\n",
    "    cnf_test = [convert2cnf(t) for t in cnf_test]\n",
    "    return cnf_train, cnf_test\n",
    "def convert2cnf(original_tree):\n",
    "    '''\n",
    "    Chomsky norm form\n",
    "    '''\n",
    "    tree = copy.deepcopy(original_tree)\n",
    "    \n",
    "    #Remove cases like NP->DT, VP->NP\n",
    "    tree.collapse_unary(collapsePOS=True, collapseRoot=True)\n",
    "    #Convert to Chomsky\n",
    "    tree.chomsky_normal_form()\n",
    "    \n",
    "    tree = set_leave_lower(tree)\n",
    "    return tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "### GET TRAIN/TEST DATA\n",
    "cnf_train, cnf_test = get_train_test_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  (NP-SBJ\n",
      "    (NP (NNP pierre) (NNP vinken))\n",
      "    (NP-SBJ|<,-ADJP-,>\n",
      "      (, ,)\n",
      "      (NP-SBJ|<ADJP-,>\n",
      "        (ADJP (NP (CD 61) (NNS years)) (JJ old))\n",
      "        (, ,))))\n",
      "  (S|<VP-.>\n",
      "    (VP\n",
      "      (MD will)\n",
      "      (VP\n",
      "        (VB join)\n",
      "        (VP|<NP-PP-CLR-NP-TMP>\n",
      "          (NP (DT the) (NN board))\n",
      "          (VP|<PP-CLR-NP-TMP>\n",
      "            (PP-CLR\n",
      "              (IN as)\n",
      "              (NP\n",
      "                (DT a)\n",
      "                (NP|<JJ-NN> (JJ nonexecutive) (NN director))))\n",
      "            (NP-TMP (NNP nov.) (CD 29))))))\n",
      "    (. .)))\n"
     ]
    }
   ],
   "source": [
    "cnf_train[0].pprint()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1\n",
    "\n",
    "To  better  understand  PCFG,  let’s  consider  the  first  parse  tree  in  the training data “cnf_train” as an example.  Run the code we have provided for you and then writedown the roles of.productions(), .rhs(), .lhs(), .leaves()in the ipynb notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[S -> NP-SBJ S|<VP-.>, NP-SBJ -> NP NP-SBJ|<,-ADJP-,>, NP -> NNP NNP, NNP -> 'pierre', NNP -> 'vinken', NP-SBJ|<,-ADJP-,> -> , NP-SBJ|<ADJP-,>, , -> ',', NP-SBJ|<ADJP-,> -> ADJP ,, ADJP -> NP JJ, NP -> CD NNS, CD -> '61', NNS -> 'years', JJ -> 'old', , -> ',', S|<VP-.> -> VP ., VP -> MD VP, MD -> 'will', VP -> VB VP|<NP-PP-CLR-NP-TMP>, VB -> 'join', VP|<NP-PP-CLR-NP-TMP> -> NP VP|<PP-CLR-NP-TMP>, NP -> DT NN, DT -> 'the', NN -> 'board', VP|<PP-CLR-NP-TMP> -> PP-CLR NP-TMP, PP-CLR -> IN NP, IN -> 'as', NP -> DT NP|<JJ-NN>, DT -> 'a', NP|<JJ-NN> -> JJ NN, JJ -> 'nonexecutive', NN -> 'director', NP-TMP -> NNP CD, NNP -> 'nov.', CD -> '29', . -> '.'] <class 'nltk.grammar.Production'>\n"
     ]
    }
   ],
   "source": [
    "rules = cnf_train[0].productions()\n",
    "print(rules, type(rules[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((NP-SBJ, S|<VP-.>), nltk.grammar.Nonterminal)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rules[0].rhs(), type(rules[0].rhs()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(('61',), str)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rules[10].rhs(), type(rules[10].rhs()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(S, nltk.grammar.Nonterminal)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rules[0].lhs(), type(rules[0].lhs())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['pierre', 'vinken', ',', '61', 'years', 'old', ',', 'will', 'join', 'the', 'board', 'as', 'a', 'nonexecutive', 'director', 'nov.', '29', '.']\n"
     ]
    }
   ],
   "source": [
    "print(cnf_train[0].leaves())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ANSWER HERE\n",
    "- productions(): This returns a grammar production where each production maps a single symbol on the \"left-hand side\" to a sequence of symbols on the \"right-hand side\". Since the grammer is in Chomsky normal form (CNF), the left-hand side must be a non-terminal symbol and the right-hand side is either a terminal symbol or two non-terminal symbols.\n",
    "- rhs(): This returns either a terminal symbol or two non-terminal symbols on the right-hand side of the production.\n",
    "- lhs(): This returns the non-terminal symbol on the left-hand side of the production.\n",
    "- leaves(): This returns the leaves of the parse tree which are terminal symbols. The order reflects the order of the leaves in the tree's hierarchical structure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2\n",
    "To count the number of unique rules, nonterminals and terminals, pleaseimplement functions **collect_rules, collect_nonterminals, collect_terminals**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_rules(train_data):\n",
    "    '''\n",
    "    Collect the rules that appear in data.\n",
    "    params:\n",
    "        train_data: list[Tree] --- list of Tree objects\n",
    "    return:\n",
    "        rules: list[nltk.grammar.Production] --- list of rules (Production objects)\n",
    "        rules_counts: Counter object --- a dictionary that maps one rule (nltk.Nonterminal) to its number of \n",
    "                                         occurences (int) in train data.\n",
    "    '''\n",
    "    rules = list()\n",
    "    rules_counts = Counter()\n",
    "    ### YOUR CODE HERE (~ 2 lines)\n",
    "    \n",
    "    for r in train_data:        \n",
    "        for rule in r.productions():          \n",
    "            rules.append(rule)        \n",
    "            rules_counts[rule] += 1              \n",
    "            \n",
    "    ### YOUR CODE HERE\n",
    "    return rules, rules_counts\n",
    "\n",
    "def collect_nonterminals(rules):\n",
    "    '''\n",
    "    collect nonterminals that appear in the rules\n",
    "    params:\n",
    "        rules: list[nltk.grammar.Production] --- list of rules (Production objects)\n",
    "    return:\n",
    "        nonterminals: set(nltk.Nonterminal) --- set of nonterminals \n",
    "    '''\n",
    "    nonterminals = list()\n",
    "    ### YOUR CODE HERE (at least one line)\n",
    "    \n",
    "    for rule in rules:\n",
    "        nonterminals.append(rule.lhs())    \n",
    "        for i in rule.rhs():           \n",
    "            if type(i) == nltk.grammar.Nonterminal:\n",
    "                nonterminals.append(i)        \n",
    "        \n",
    "    ### END OF YOUR CODE\n",
    "    return set(nonterminals)\n",
    "\n",
    "def collect_terminals(rules):\n",
    "    '''\n",
    "    collect terminals that appear in the rules\n",
    "    params:\n",
    "        rules: list[nltk.grammar.Production] --- list of rules (Production objects)\n",
    "    return:\n",
    "        terminals: set of strings --- set of terminals    \n",
    "    '''\n",
    "    terminals = list()\n",
    "    ### YOUR CODE HERE (at least one line)\n",
    "    \n",
    "    for rule in rules:\n",
    "        for i in rule.rhs():\n",
    "            if type(i) != nltk.grammar.Nonterminal:\n",
    "                terminals.append(i) \n",
    "    \n",
    "    ### END OF YOUR CODE\n",
    "\n",
    "    return set(terminals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_rules, train_rules_counts = collect_rules(cnf_train)\n",
    "nonterminals = collect_nonterminals(train_rules)\n",
    "terminals = collect_terminals(train_rules)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(196646, 31656, 11367, 7869)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### CORRECT ANSWER (19xxxx, 3xxxx, 1xxxx, 7xxx)\n",
    "len(train_rules), len(set(train_rules)), len(terminals), len(nonterminals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(, -> ',', 4876), (DT -> 'the', 4726), (. -> '.', 3814), (PP -> IN NP, 3273), (S|<VP-.> -> VP ., 3003)]\n"
     ]
    }
   ],
   "source": [
    "print(train_rules_counts.most_common(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 3\n",
    "Implement the function **build_pcfg** which builds a dictionary that stores theterminal rules and nonterminal rules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_pcfg(rules_counts):\n",
    "    '''\n",
    "    Build a dictionary that stores the terminal rules and nonterminal rules.\n",
    "    param:\n",
    "        rules_counts: Counter object --- a dictionary that maps one rule to its number of occurences in train data.\n",
    "    return:\n",
    "        rules_dict: dict(dict(dict)) --- a dictionary has a form like:\n",
    "                    rules_dict = {'terminals':{'NP':{'the':1000,'an':500}, 'ADJ':{'nice':500,'good':100}},\n",
    "                                  'nonterminals':{'S':{'NP@VP':1000},'NP':{'NP@NP':540}}}\n",
    "    When building \"rules_dict\", you need to use \"lhs()\", \"rhs()\" funtion and convert Nonterminal to str.\n",
    "    All the keys in the dictionary are of type str.\n",
    "    '@' is used as a special symbol to split left and right nonterminal strings.\n",
    "    '''\n",
    "    \n",
    "    rules_dict = dict()\n",
    "    ### rules_dict['terminals'] contains rules like \"NP->'the'\"\n",
    "    ### rules_dict['nonterminals'] contains rules like \"S->NP@VP\"\n",
    "    rules_dict['terminals'] = defaultdict(dict)\n",
    "    rules_dict['nonterminals'] = defaultdict(dict)\n",
    "    \n",
    "    ### YOUR CODE HERE\n",
    "    for key in rules_counts.keys():        \n",
    "        if type(key.rhs()[0]) != nltk.grammar.Nonterminal:\n",
    "            rules_dict['terminals'][key.lhs().__str__()][key.rhs()[0].__str__()] = rules_counts[key]\n",
    "        else:\n",
    "            new_key = key.rhs()[0].__str__() + \"@\" + key.rhs()[1].__str__() \n",
    "            rules_dict['nonterminals'][key.lhs().__str__()][new_key] = rules_counts[key]               \n",
    "    \n",
    "    ### END OF YOUR CODE\n",
    "    return rules_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_rules_dict = build_pcfg(train_rules_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 4\n",
    "Estimate the probability of rule $NP\\rightarrow NNP@NNP$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The estimated probability of rule NP→NNP@NNP is 0.03950843529348353\n"
     ]
    }
   ],
   "source": [
    "count_np_nnp_nnp = train_rules_dict['nonterminals']['NP']['NNP@NNP'] \n",
    "count_np = 0\n",
    "for key in train_rules_dict['nonterminals']['NP']:\n",
    "    count_np += train_rules_dict['nonterminals']['NP'][key]\n",
    "prob = count_np_nnp_nnp / count_np\n",
    "print(\"The estimated probability of rule NP→NNP@NNP is {}\".format(prob))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 5\n",
    "Find the terminal symbols in ''cnf_test[0]'' that never appeared in the PCFG we built. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "constitutional-law\n"
     ]
    }
   ],
   "source": [
    "test_rules, test_rules_counts = collect_rules([cnf_test[0]])\n",
    "test_terminals = collect_terminals(test_rules)\n",
    "for test_terminal in test_terminals:\n",
    "    if test_terminal not in terminals:\n",
    "        print(test_terminal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 6\n",
    "We can use smoothing techniques to handle these cases. A simple smoothing method is as follows. We first create a new \"unknown\" terminal symbol $unk$.\n",
    "\n",
    "Next, for each original non-terminal symbol $A\\in N$, we add one new rule $A \\rightarrow unk$ to the original PCFG.\n",
    "\n",
    "The smoothed probabilities for all rules can then be estimated as:\n",
    "$$q_{smooth}(A \\rightarrow \\beta) = \\frac {count(A \\rightarrow \\beta)}{count(A)+1}$$\n",
    "$$q_{smooth}(A \\rightarrow unk) = \\frac {1}{count(A)+1}$$\n",
    "where $|V|$ is the count of unique terminal symbols.\n",
    "\n",
    "\n",
    "Implement the function **smooth_rules_prob** which returns the smoothed rule probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def smooth_rules_prob(rules_counts):\n",
    "    '''\n",
    "    params:\n",
    "        rules_counts: dict(dict(dict)) --- a dictionary has a form like:\n",
    "                      rules_counts = {'terminals':{'NP':{'the':1000,'an':500}, 'ADJ':{'nice':500,'good':100}},\n",
    "                                      'nonterminals':{'S':{'NP@VP':1000},'NP':{'NP@NP':540}}}\n",
    "    \n",
    "    return:\n",
    "        rules_prob: dict(dict(dict)) --- a dictionary that has a form like:\n",
    "                               rules_prob = {'terminals':{'NP':{'the':0.6,'an':0.3, '<unk>':0.1},\n",
    "                                                          'ADJ':{'nice':0.6,'good':0.3,'<unk>':0.1},\n",
    "                                                          'S':{'<unk>':0.01}}}\n",
    "                                             'nonterminals':{'S':{'NP@VP':0.99}}\n",
    "    '''\n",
    "    rules_prob = copy.deepcopy(rules_counts)\n",
    "    unk = '<unk>'\n",
    "    ### Hint: don't forget to consider nonterminal symbols that don't appear in rules_counts['terminals'].keys()\n",
    "    ### YOUR CODE HERE\n",
    "    \n",
    "    all_nonterminal_symbols = set(list(rules_prob['nonterminals'].keys()) + list(rules_prob['terminals'].keys())) \n",
    "\n",
    "    for lhs in all_nonterminal_symbols:\n",
    "        countA = 0\n",
    "        if lhs in rules_prob['terminals'].keys():            \n",
    "            for rhs in rules_prob['terminals'][lhs]:\n",
    "                countA += rules_prob['terminals'][lhs][rhs]\n",
    "                \n",
    "        if lhs in rules_prob['nonterminals'].keys():             \n",
    "            for rhs in rules_prob['nonterminals'][lhs]:\n",
    "                countA += rules_prob['nonterminals'][lhs][rhs]  \n",
    "        \n",
    "        if lhs in rules_prob['terminals'].keys():\n",
    "            for rhs in rules_prob['terminals'][lhs]:\n",
    "                countA_Beta = rules_prob['terminals'][lhs][rhs]\n",
    "                rules_prob['terminals'][lhs][rhs] = countA_Beta/(countA + 1.0)\n",
    "\n",
    "        if lhs in rules_prob['nonterminals'].keys():  \n",
    "            for rhs in rules_prob['nonterminals'][lhs]:\n",
    "                countA_Beta = rules_prob['nonterminals'][lhs][rhs] \n",
    "                rules_prob['nonterminals'][lhs][rhs] = countA_Beta/(countA + 1.0)\n",
    "                \n",
    "        rules_prob['terminals'][lhs][unk] = 1.0/(countA + 1.0)\n",
    "            \n",
    "    ### END OF YOUR CODE\n",
    "    return rules_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "s_rules_prob = smooth_rules_prob(train_rules_dict)\n",
    "terminals.add('<unk>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1300172371337109\n",
      "0.025240088648116228\n",
      "0.039506305917861376\n",
      "{'<unk>': 5.389673385792821e-05}\n"
     ]
    }
   ],
   "source": [
    "print(s_rules_prob['nonterminals']['S']['NP-SBJ@S|<VP-.>'])\n",
    "print(s_rules_prob['nonterminals']['S']['NP-SBJ-1@S|<VP-.>'])\n",
    "print(s_rules_prob['nonterminals']['NP']['NNP@NNP'])\n",
    "print(s_rules_prob['terminals']['NP'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11368"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(terminals)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CKY Algorithm\n",
    "\n",
    "Similar to the Viterbi algorithm, the CKY algorithm is a dynamic-programming algorithm. Given a PCFG $G=(N, \\ \\Sigma, \\ S, \\ R, \\ q)$, we can use the CKY algorithm described in class to find the highest scoring parse tree for a sentence. \n",
    "\n",
    "First, let us complete the *CKY* function from scratch using only Python built-in functions and the Numpy package. \n",
    "\n",
    "The output should be two dictionaries $\\pi$ and $bp$, which store the optimal probability and backpointer information respectively.\n",
    "\n",
    "Given a sentence $w_0, w_1, ...,w_{n-1}$,  $\\pi(i, k, X)$, $bp(i, k, X)$ refer to the highest score and backpointer for the (partial) parse tree that has the root X (a non-terminal symbol) and covers the word span $w_i, ..., w_{k-1}$, where $0 \\le i < k \\le n$. Note that a backpointer includes both the best grammar rule chosen and the best split point.\n",
    "![tree](imgs/parse_tree.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 7\n",
    "Implement **CKY** function and run the test code to check your implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CKY(sent, rules_prob):\n",
    "    '''\n",
    "    params:\n",
    "        sent: list[str] --- a list of strings\n",
    "        rules_prob: dict(dict(dict)) --- a dictionary that has a form like:\n",
    "                                           rules_prob = {'terminals':{'NP':{'the':0.6,'an':0.3, '<unk>':0.1},\n",
    "                                                                      'ADJ':{'nice':0.6,'good':0.3,'<unk>':0.1},\n",
    "                                                                      'S':{'<unk>':0.01}}}\n",
    "                                                         'nonterminals':{'S':{'NP@VP':0.99}}\n",
    "    return:\n",
    "        score: dict() --- score[(i,i+span)][root] represents the highest score for the parse (sub)tree that has the root \"root\"\n",
    "                          across words w_i, w_{i+1},..., w_{i+span-1}.\n",
    "        back: dict() --- back[(i,i+span)][root] = (split , left_child, right_child); split: int; \n",
    "                         left_child: str; right_child: str. \n",
    "    '''\n",
    "    score = defaultdict(dict)\n",
    "    back = defaultdict(dict)\n",
    "    sent_len = len(sent)\n",
    "    ### YOUR CODE HERE\n",
    "    for i in range(0,sent_len):\n",
    "        for A in rules_prob['terminals'].keys():\n",
    "            if sent[i] in rules_prob['terminals'][A].keys():\n",
    "                score[(i,i+1)][A] = rules_prob['terminals'][A][sent[i]]\n",
    "                back[(i,i+1)][A] = ('terminal', sent[i])\n",
    "\n",
    "    for span in range(2, sent_len+1):\n",
    "        for begin in range(0, sent_len - span + 1):\n",
    "            end = begin + span\n",
    "            for split in range(begin+1, end):\n",
    "                for A in rules_prob['nonterminals'].keys():                    \n",
    "                    for rhs in rules_prob['nonterminals'][A].keys():         \n",
    "                        B = rhs.split('@')[0]\n",
    "                        C = rhs.split('@')[1]  \n",
    "                        if B in score[(begin, split)].keys() and C in score[(split, end)].keys():\n",
    "                            new_score = rules_prob['nonterminals'][A][rhs] * score[(begin, split)][B] * score[(split, end)][C]\n",
    "                            if A not in score[(begin, end)] or new_score > score[(begin, end)][A]:\n",
    "                                score[(begin, end)][A] = new_score\n",
    "                                back[(begin, end)][A] = (split, B, C)                   \n",
    "\n",
    "\n",
    "    ### END OF YOUR CODE                  \n",
    "    return score, back"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent = cnf_train[0].leaves()\n",
    "score, back = CKY(sent, s_rules_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9.135335125206641e-52"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score[(0, len(sent))]['S']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 8\n",
    "Implement **build_tree** function according to algorithm 2 to reconstruct theparse tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_tree(back, root):\n",
    "    '''\n",
    "    Build the tree recursively.\n",
    "    params:\n",
    "        back: dict() --- back[(i,i+span)][X] = (split , left_child, right_child); split:int; left_child: str; right_child: str.\n",
    "        root: tuple() --- (begin, end, nonterminal_symbol), e.g., (0, 10, 'S\n",
    "    return:\n",
    "        tree: nltk.tree.Tree\n",
    "    '''\n",
    "    begin = root[0]\n",
    "    end = root[1]\n",
    "    root_label = root[2]\n",
    "    ### YOUR CODE HERE  \n",
    "    \n",
    "    back_output = back[(begin, end)][root_label]\n",
    "    if back_output[0] == 'terminal':\n",
    "        tree = Tree(root_label, [back_output[1]])    \n",
    "    else:\n",
    "        split, B, C = back_output\n",
    "        left = build_tree(back, (begin, split, B))\n",
    "        right = build_tree(back, (split, end, C))\n",
    "        tree = Tree(root_label, [left, right])            \n",
    "\n",
    "    ### END OF YOUR CODE \n",
    "    return tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  (NP-SBJ\n",
      "    (NP (NNP pierre) (NNP vinken))\n",
      "    (NP-SBJ|<,-NP-,>\n",
      "      (, ,)\n",
      "      (NP-SBJ|<NP-,>\n",
      "        (NP (CD 61) (NP|<NNS-JJ> (NNS years) (JJ old)))\n",
      "        (, ,))))\n",
      "  (S|<VP-.>\n",
      "    (VP\n",
      "      (MD will)\n",
      "      (VP\n",
      "        (VB join)\n",
      "        (VP|<NP-PP-CLR-NP-TMP>\n",
      "          (NP (DT the) (NN board))\n",
      "          (VP|<PP-CLR-NP-TMP>\n",
      "            (PP-CLR\n",
      "              (IN as)\n",
      "              (NP\n",
      "                (DT a)\n",
      "                (NP|<JJ-NN> (JJ nonexecutive) (NN director))))\n",
      "            (NP-TMP (NNP nov.) (CD 29))))))\n",
      "    (. .)))\n"
     ]
    }
   ],
   "source": [
    "build_tree(back, (0, len(sent), 'S')).pprint()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_leave_index(tree):\n",
    "    '''\n",
    "    Label the leaves of the tree with indexes\n",
    "    Arg:\n",
    "        tree: original tree, nltk.tree.Tree\n",
    "    Return:\n",
    "        tree: preprocessed tree, nltk.tree.Tree\n",
    "    '''\n",
    "    for idx, _ in enumerate(tree.leaves()):\n",
    "        tree_location = tree.leaf_treeposition(idx)\n",
    "        non_terminal = tree[tree_location[:-1]]\n",
    "        non_terminal[0] = non_terminal[0] + \"_\" + str(idx)\n",
    "    return tree\n",
    "\n",
    "def get_nonterminal_bracket(tree):\n",
    "    '''\n",
    "    Obtain the constituent brackets of a tree\n",
    "    Arg:\n",
    "        tree: tree, nltk.tree.Tree\n",
    "    Return:\n",
    "        nonterminal_brackets: constituent brackets, set\n",
    "    '''\n",
    "    nonterminal_brackets = set()\n",
    "    for tr in tree.subtrees():\n",
    "        label = tr.label()\n",
    "        #print(tr.leaves())\n",
    "        if len(tr.leaves()) == 0:\n",
    "            continue\n",
    "        start = tr.leaves()[0].split('_')[-1]\n",
    "        end = tr.leaves()[-1].split('_')[-1]\n",
    "        if start != end:\n",
    "            nonterminal_brackets.add(label+'-('+start+':'+end+')')\n",
    "    return nonterminal_brackets\n",
    "\n",
    "def word2lower(w, terminals):\n",
    "    '''\n",
    "    Map an unknow word to \"unk\"\n",
    "    '''\n",
    "    return w.lower() if w in terminals else '<unk>'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "####################\n",
      "Test Tree: 1\n",
      "Constituent number in the predicted tree: 20\n",
      "Constituent number in the gold tree: 20\n",
      "Correct constituent number: 14\n",
      "####################\n",
      "Test Tree: 2\n",
      "Constituent number in the predicted tree: 54\n",
      "Constituent number in the gold tree: 54\n",
      "Correct constituent number: 30\n",
      "####################\n",
      "Test Tree: 3\n",
      "Constituent number in the predicted tree: 30\n",
      "Constituent number in the gold tree: 30\n",
      "Correct constituent number: 23\n",
      "####################\n",
      "Test Tree: 4\n",
      "Constituent number in the predicted tree: 17\n",
      "Constituent number in the gold tree: 17\n",
      "Correct constituent number: 16\n",
      "####################\n",
      "Test Tree: 5\n",
      "Constituent number in the predicted tree: 32\n",
      "Constituent number in the gold tree: 32\n",
      "Correct constituent number: 26\n",
      "####################\n",
      "Test Tree: 6\n",
      "Constituent number in the predicted tree: 40\n",
      "Constituent number in the gold tree: 40\n",
      "Correct constituent number: 18\n",
      "####################\n",
      "Test Tree: 7\n",
      "Constituent number in the predicted tree: 22\n",
      "Constituent number in the gold tree: 22\n",
      "Correct constituent number: 7\n",
      "####################\n",
      "Test Tree: 8\n",
      "Constituent number in the predicted tree: 18\n",
      "Constituent number in the gold tree: 18\n",
      "Correct constituent number: 6\n",
      "####################\n",
      "Test Tree: 9\n",
      "Constituent number in the predicted tree: 28\n",
      "Constituent number in the gold tree: 28\n",
      "Correct constituent number: 16\n",
      "####################\n",
      "Test Tree: 10\n",
      "Constituent number in the predicted tree: 40\n",
      "Constituent number in the gold tree: 40\n",
      "Correct constituent number: 8\n"
     ]
    }
   ],
   "source": [
    "correct_count = 0\n",
    "pred_count = 0\n",
    "gold_count = 0\n",
    "for i, t in enumerate(cnf_test):\n",
    "    #Protect the original tree \n",
    "    t = copy.deepcopy(t)\n",
    "    sent = t.leaves()  \n",
    "    #Map the unknow words to \"unk\"\n",
    "    sent = [word2lower(w.lower(), terminals) for w in sent]\n",
    "    \n",
    "    #CKY algorithm\n",
    "    score, back = CKY(sent, s_rules_prob)\n",
    "    candidate_tree = build_tree(back, (0, len(sent), 'S'))\n",
    "    \n",
    "    #Extract constituents from the gold tree and predicted tree\n",
    "    pred_tree = set_leave_index(candidate_tree)\n",
    "    pred_brackets = get_nonterminal_bracket(pred_tree)\n",
    "    \n",
    "    #Count correct constituents\n",
    "    pred_count += len(pred_brackets)\n",
    "    gold_tree = set_leave_index(t)\n",
    "    gold_brackets = get_nonterminal_bracket(gold_tree)\n",
    "    gold_count += len(gold_brackets)\n",
    "    current_correct_num = len(pred_brackets.intersection(gold_brackets))\n",
    "    correct_count += current_correct_num\n",
    "    \n",
    "    print('#'*20)\n",
    "    print('Test Tree:', i+1)\n",
    "    print('Constituent number in the predicted tree:', len(pred_brackets))\n",
    "    print('Constituent number in the gold tree:', len(gold_brackets))\n",
    "    print('Correct constituent number:', current_correct_num)\n",
    "\n",
    "recall = correct_count/gold_count\n",
    "precision = correct_count/pred_count\n",
    "f1 = 2*recall*precision/(recall+precision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall precision: 0.545, recall: 0.545, f1: 0.545\n"
     ]
    }
   ],
   "source": [
    "print('Overall precision: {:.3f}, recall: {:.3f}, f1: {:.3f}'.format(precision, recall, f1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall precision: 0.545, recall: 0.545, f1: 0.545\n"
     ]
    }
   ],
   "source": [
    "print('Overall precision: {:.3f}, recall: {:.3f}, f1: {:.3f}'.format(precision, recall, f1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "782.2465057373047\n"
     ]
    }
   ],
   "source": [
    "et=time.time()\n",
    "print(et - st)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
